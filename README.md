# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Дубских Семён Николаевич
- РИ210950
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Интеграция экономической системы в проект Unity и обучение ML-Agent.

## Задание 1
### Измените параметры файла .yaml-агента и определить какие параметры и как влияют на обучение модели.

* Параметры по умолчанию. График `Cumulative Reward` возрастает монотонно верх, в виде логарифмической функции:

![до изменений](https://user-images.githubusercontent.com/45539357/204099825-6908e57d-9b0d-40d1-94d1-55b1a182c1c0.png)

* Параметр `learning_rate = 1.0e-5` соответствуют начальной скорости обучения для градиентного спуска. 
График сначала экспоненциально возрастает, дальше убывает. 
Другими словами вознаграждение за обучение достигло пика и начало опускаться:

![learning_rate1e-5](https://user-images.githubusercontent.com/45539357/204099751-c8118901-6325-435c-84cd-d609d21b308e.png)

* Параметр `beta = 1.0e-4` соответствует силе энтропийной регуляризации, которая делает политику «более случайной».
Обучение вознаграждалось за константу на каждом шаге, график прямой, стабильные обновления:

![beta1e-4](https://user-images.githubusercontent.com/45539357/204105291-b4bcebbd-e012-4df6-95f6-5cac838af533.png)

* Параметр `epsilon = 0.3` влияет на то, насколько быстро политика может развиваться во время обучения. График растёт вверх линейно, но медленно

![epsilon0 3](https://user-images.githubusercontent.com/45539357/204105720-3fdfae0f-246f-4b3e-b501-5dd29dfb2260.png)

* Параметр `lambd = 0.9` используется при расчете обобщенной оценки преимущества (GAE). График похож на параболу с ветвями вниз, обучение не стабильное:

![lambd0 9](https://user-images.githubusercontent.com/45539357/204106905-2dcdfe5d-9665-4023-bd45-9292a26c21c4.png)

* Параметр `num_epoch = 10` - количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. Логарифмический график функции, не стабильные обновления:

![num_epoch10](https://user-images.githubusercontent.com/45539357/204107436-3b92e54e-0c7a-4747-9c9e-eb61a995d28b.png)

## Задание 2
### Опишите результаты, выведенные в TensorBoard.

Графики, представленные в предыдущем задании означают следующее:

* `Cumulative reward` - cреднее совокупное вознаграждение за эпизод для всех агентов. Общая тенденция вознаграждения должна постоянно увеличиваться с течением времени.
Могут наблюдаться небольшие взлеты и падения. В зависимости от сложности задачи значительное увеличение вознаграждения может не проявиться до тех пор, пока не будут пройдены миллионы шагов в процессе обучения.
* `Episode Length` - cредняя продолжительность каждого эпизода в среде для всех агентов.
* `Policy Loss` - средняя величина функции `policy loss`. Соотносится с тем, насколько сильно меняется политика (процесс принятия решений).
Разброс этой величины должен уменьшаться во время успешной тренировки. Значения будут колебаться, но как правило, должны быть меньше 1,0.
* `Value Loss` - cредняя потеря обновления функции значения. Коррелирует с тем, насколько хорошо модель способна предсказать значение каждого состояния. 
Должна увеличиваться, пока агент обучается, а затем уменьшаться, когда вознаграждение стабилизируется.

Изменяя параметры в файле `.yaml` конфигурации, обучение может ускориться или замедлиться, может быть стабильнее или более случайным. 
Кроме того для разных тренажёров могут быть разные параметры, которые так же влияют на процесс обучения модели.

## Выводы

В ходе лабораторной работы я ознакомился с возможностью интеграции экономической системы в проект Unity, а также использованием её для обучения ML-Agent.
Разобрался как изменение параметров конфигурации модели влияет на её обучение, а также на производительность работы. Понял каким образом представлены графики в
`tensorboard`, что они означают и как их анализировать.

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
